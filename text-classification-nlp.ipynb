{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","\n","import matplotlib as mpl \n","import matplotlib.cm as cm \n","import matplotlib.pyplot as plt \n","import plotly.graph_objects as go\n","import seaborn as sns\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction import stop_words\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer \n","\n","import string\n","import re\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","\n","from sklearn.metrics import accuracy_score\n","import sklearn.metrics as metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix \n","from sklearn.metrics import classification_report\n","from sklearn import metrics\n","\n","from time import time\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["## Analysis of Data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["data = pd.read_csv('../input/us-economic-news-articles/US-Economic-News.csv', encoding = 'ISO-8859-1')\n","display(data.shape) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data[\"relevance\"].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["There is an imbalance in the data with not relevant being 82% in the dataset. That is, most of the articles are not relevant to US Economy, which makes sense in a real-world scenario, as news articles discuss various topics. We should keep this class imbalance mind when interpreting the classifier performance later. Let us first convert the class labels into binary outcome variables for convenience. 1 for Yes (relevant), and 0 for No (not relevant), and ignore \"Not sure\"."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = data[data.relevance != \"not sure\"]\n","data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data[\"relevance\"].value_counts()/data.shape[0] "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = go.Figure([go.Bar(x=data['relevance'].value_counts().index, y=data['relevance'].value_counts().tolist())])\n","fig.update_layout(\n","    title=\"Values in each Sentiment\",\n","    xaxis_title=\"Sentiment\",\n","    yaxis_title=\"Values\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Text Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data['relevance'] = data.relevance.map({'yes':1, 'no':0}) # relevant is 1, not-relevant is 0 \n","data = data[[\"text\",\"relevance\"]] # taking text input and output variable as relevance\n","data = data[:1000]\n","data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data['text'][0]"]},{"cell_type":"markdown","metadata":{},"source":["### Text Cleaning"]},{"cell_type":"markdown","metadata":{},"source":["Typical steps involve tokenization, lower casing, removing, stop words, punctuation markers etc, and vectorization. Other processes such as stemming/lemmatization can also be performed. Here, we are performing the following steps: removing br tags, punctuation, numbers, and stopwords. While we are using sklearn's list of stopwords, there are several other stop word lists (e.g., from NLTK) or sometimes, custom stopword lists are needed depending on the task."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","stopwords = stop_words.ENGLISH_STOP_WORDS\n","lemmatizer = WordNetLemmatizer()\n","\n","def clean(doc):\n","    text_no_namedentities = []\n","    document = nlp(doc)\n","    ents = [e.text for e in document.ents]\n","    for item in document:\n","        if item.text in ents:\n","            pass\n","        else:\n","            text_no_namedentities.append(item.text)\n","    doc = (\" \".join(text_no_namedentities))\n","\n","    doc = doc.lower().strip()\n","    doc = doc.replace(\"</br>\", \" \") \n","    doc = doc.replace(\"-\", \" \") \n","    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n","    doc = \" \".join([token for token in doc.split() if token not in stopwords])    \n","    doc = \"\".join([lemmatizer.lemmatize(word) for word in doc])\n","    return doc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clean(data['text'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data['text'] = data['text'].apply(clean)\n","data.head()"]},{"cell_type":"markdown","metadata":{},"source":["Now we are ready for the modeling. We are going to use algorithms from sklearn package. We will go through the following steps:\n","\n","1. Split the data into training and test sets (80% train, 20% test)\n","2. Extract features from the training data using TfidfVectorizer.\n","3. Transform the test data into the same feature vector as the training data.\n","4. Train the classifier\n","5. Evaluate the classifier"]},{"cell_type":"markdown","metadata":{},"source":["## TF-IDF Vectorizer\n","\n","![TF-IDF](https://miro.medium.com/max/3136/1*ruCawEw0--m2SeHmAQooJQ.jpeg)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["docs = list(data['text'])\n","tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features = 20000) \n","tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\n","docs = tfidf_vectorizer_vectors.toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X = docs \n","y = data['relevance']\n","print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = go.Figure([go.Bar(x=y.value_counts().index, y=y.value_counts().tolist())])\n","fig.update_layout(\n","    title=\"Values in each Sentiment\",\n","    xaxis_title=\"Sentiment\",\n","    yaxis_title=\"Values\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Train-Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SEED=123\n","X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Naive Bayes Classifier"]},{"cell_type":"markdown","metadata":{},"source":["### Gaussian Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gnb = GaussianNB() \n","%time gnb.fit(X_train, y_train)\n","\n","y_pred_train = gnb.predict(X_train)\n","y_pred_test = gnb.predict(X_test)\n","print(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\n","print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred_test)\n","# print('Confusion matrix\\n', cm)\n","\n","cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n","                        index=['Predict Positive', 'Predict Negative'])\n","sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["probs = gnb.predict_proba(X_test)\n","preds = probs[:,1]\n","fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n","roc_auc = metrics.auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Gaussian Naive Bayes performs poorly in this case because of the prior and posterior probability condition"]},{"cell_type":"markdown","metadata":{},"source":["### Multinomial Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mnb = MultinomialNB() \n","%time mnb.fit(X_train, y_train)\n","\n","y_pred_train = mnb.predict(X_train)\n","y_pred_test = mnb.predict(X_test)\n","print(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\n","print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred_test)\n","# print('Confusion matrix\\n', cm)\n","\n","cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n","                        index=['Predict Positive', 'Predict Negative'])\n","sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["probs = mnb.predict_proba(X_test)\n","preds = probs[:,1]\n","fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n","roc_auc = metrics.auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Multinomial Naive Bayes performs slightly better than Gaussian Naive Bayes, but the low AUC score is because the size of feature vector is really big and Bayes Algorothm works better for small number of features. Let's check out results of Logistic Regression, Support Vector Machines and Decision Tree Classifier."]},{"cell_type":"markdown","metadata":{},"source":["## Logistic Regression Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr = LogisticRegression(random_state=SEED)\n","%time lr.fit(X_train, y_train)\n","\n","y_pred_train = lr.predict(X_train)\n","y_pred_test = lr.predict(X_test)\n","print(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\n","print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred_test)\n","#print('Confusion matrix\\n', cm)\n","\n","cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n","                        index=['Predict Positive', 'Predict Negative'])\n","sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["probs = lr.predict_proba(X_test)\n","preds = probs[:,1]\n","fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n","roc_auc = metrics.auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Support Vector Machines"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["svc =  LinearSVC(class_weight='balanced') \n","%time svc.fit(X_train, y_train)\n","\n","y_pred_train = svc.predict(X_train)\n","y_pred_test = svc.predict(X_test)\n","print(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\n","print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred_test)\n","# print('Confusion matrix\\n', cm)\n","\n","cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n","                        index=['Predict Positive', 'Predict Negative'])\n","sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["probs = svc._predict_proba_lr(X_test)\n","preds = probs[:,1]\n","fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n","roc_auc = metrics.auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Decision Tree Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","dt = DecisionTreeClassifier(random_state=SEED)\n","%time dt.fit(X_train, y_train)\n","\n","y_pred_train = dt.predict(X_train)\n","y_pred_test = dt.predict(X_test)\n","print(\"\\nTraining Accuracy score:\",accuracy_score(y_train, y_pred_train))\n","print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred_test)\n","# print('Confusion matrix\\n', cm)\n","\n","cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n","                        index=['Predict Positive', 'Predict Negative'])\n","sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(classification_report(y_test, y_pred_test, target_names=['not relevant', 'relevant']))"]},{"cell_type":"markdown","metadata":{},"source":["## Ensembling"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.ensemble import VotingClassifier\n","\n","classifiers = [('Decision Tree', dt),\n","               ('Logistic Regression', lr),\n","                ('Naive Bayes', gnb)\n","              ]\n","vc = VotingClassifier(estimators=classifiers)\n","# Fit 'vc' to the traing set and predict test set labels\n","vc.fit(X_train, y_train)\n","y_pred_train=vc.predict(X_train)\n","y_pred_test = vc.predict(X_test)\n","print(\"Training Accuracy score:\",accuracy_score(y_train, y_pred_train))\n","print(\"Testing Accuracy score:\",accuracy_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = pd.Series(vc.predict(X), name=\"relevant\")\n","results = pd.concat([predictions],axis=1)\n","results.to_csv(\"us-economic-news-relevance.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["\n","So, how do we choose whats the best? If we look at overall accuracy alone, we should be choosing the very first classifier in this notebook. However, that is also doing poorly with identifying \"relevant\" articles. If we choose purely based on how good it is doing with \"relevant\" category, we should choose the second one we built. If we choose purely based on how good it is doing with \"irrelevant\" category, surely, nothing beats not building any classifier and just calling everything irrelevant! So, what to choose as the best among these depends on what we are looking for in our usecase!\n","\n","Let me know how you found this notebook and share your feedback in the comments section. Happy Kaggling :)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
